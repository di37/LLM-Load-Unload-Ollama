{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading LLM when used via Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping the model loaded in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we will see how to keep the model loaded in memory indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating the model and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3\", keep_alive=-1)\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is running for the first time. The model will be initially loaded, which takes a little bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the astronaut break up with his girlfriend?\n",
      "\n",
      "Because he needed space!\n",
      "\n",
      "(Sorry, it's a bit of a \"stellar\" pun, but I hope it launched a smile on your face!)\n",
      "\n",
      "Elapsed time: 6.09 seconds\n",
      "Current time: 02:42:13\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(chain.invoke({\"topic\": \"Space travel\"}))\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nElapsed time: {elapsed_time:.2f} seconds\")\n",
    "current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"Current time:\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the model again after 10 minutes to ensure the model is still loaded. This is to check if the model is still loaded in VRAM even after 5 minutes, which is the default time to keep the model loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the astronaut break up with his girlfriend before going to Mars?\n",
      "\n",
      "Because he needed space!\n",
      "\n",
      "Elapsed time: 0.39 seconds\n",
      "Current time: 02:52:33\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(chain.invoke({\"topic\": \"Space travel\"}))\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nElapsed time: {elapsed_time:.2f} seconds\")\n",
    "current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"Current time:\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: We can see from both runs of elapsed and current time that the model has been kept loaded in memory for more than 10 minutes, as the elapsed time was much shorter than the elapsed time of the previous run of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unloading the model immediately after inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see how the model can immediately be unloaded after inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-instatiating model and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3\", keep_alive=0)\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the astronaut break up with his girlfriend?\n",
      "\n",
      "Because he needed space! (get it?)\n",
      "\n",
      "Elapsed time: 5.81 seconds\n",
      "Current time: 02:58:35\n"
     ]
    }
   ],
   "source": [
    "## Initially model was loaded. So had to re-run this cell to ensure the unloading and reloading of the model during inferencing.\n",
    "start_time = time.time()\n",
    "print(chain.invoke({\"topic\": \"Space travel\"}))\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nElapsed time: {elapsed_time:.2f} seconds\")\n",
    "current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"Current time:\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run the model to see if the model is unloaded from the memory or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the astronaut break up with his girlfriend before going to Mars?\n",
      "\n",
      "Because he needed space!\n",
      "\n",
      "Elapsed time: 5.62 seconds\n",
      "Current time: 02:58:47\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(chain.invoke({\"topic\": \"Space travel\"}))\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nElapsed time: {elapsed_time:.2f} seconds\")\n",
    "current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"Current time:\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Elapsed time for both runs indicates that the model was immediately unloaded after the first run and then reloaded into memory for the second run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on `keep_alive` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the use case, the `keep_alive` parameter can be set to:\n",
    "- a duration string (such as \"10m\" or \"24h\")\n",
    "- a number in seconds (such as 3600)\n",
    "- any negative number which will keep the model loaded in memory (e.g. -1 or \"-1m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
