# Loading and Unloading of LLMs while using via Ollama

This simple demonstration is done to show how an LLM can be loaded and unloaded while using via Ollama.  

## Pre-requisites

Assuming that Ollama from its official website and the dependencies in `requirements.txt` are installed.

## Demonstration

Please check `run.ipynb` to see the demonstration.

## References
- https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately